{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOoXjqlG9ah//fINSnjsst",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeREXus/langchain-learnings/blob/main/Langchain_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retreivers**"
      ],
      "metadata": {
        "id": "FAke2i2iRRdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "Fc1VnOryRMZ2",
        "outputId": "6770b763-eb26-44a1-d5ca-3666c22d537c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.14) (1.3.1)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-6.33.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "fb5b63fdb51041ac807a9d708ef5f92f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf==4.3.1 in /usr/local/lib/python3.12/dist-packages (4.3.1)\n",
            "Requirement already satisfied: lark==1.1.9 in /usr/local/lib/python3.12/dist-packages (1.1.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.7->posthog<6.0.0) (2025.10.5)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain==0.2.14 langchain-community langchain-core langchain-text-splitters langchain-google-genai| tail -n 1\n",
        "!pip install \"chromadb==0.4.24\" | tail -n 1\n",
        "!pip install \"pypdf==4.3.1\" | tail -n 1\n",
        "!pip install \"lark==1.1.9\" | tail -n 1\n",
        "!pip install 'posthog<6.0.0' | tail -n 1\n",
        "!pip install 'numpy ==1.26.4' |tail -n 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MZ9z1lm-Ui3YBp3SYWLTAQ/companypolicies.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B5kwO5-9DIJ",
        "outputId": "b2862b5b-20da-4f05-ce89-57464b5b6f33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-08 14:01:58--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MZ9z1lm-Ui3YBp3SYWLTAQ/companypolicies.txt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15660 (15K) [text/plain]\n",
            "Saving to: ‘companypolicies.txt.1’\n",
            "\n",
            "companypolicies.txt 100%[===================>]  15.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-08 14:01:59 (238 MB/s) - ‘companypolicies.txt.1’ saved [15660/15660]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers |tail -n 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_P3QlQBEzZq",
        "outputId": "3d1c5f11-8de8-4bd2-ce22-871485410923"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "KjiXYLf6SFpi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n"
      ],
      "metadata": {
        "id": "5ntjPO9_S81M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_model():\n",
        "  embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "  return embed_model"
      ],
      "metadata": {
        "id": "IRezW-SnTMcJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitter( data, chunk_size , chunk_overlap):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_size,\n",
        "      chunk_overlap = chunk_overlap,\n",
        "      length_function = len\n",
        "  )\n",
        "  chunks = text_splitter.split_documents(data)\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "73RKmRTvaXzS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm():\n",
        "  llm = GoogleGenerativeAI(\n",
        "    model='gemini-1.5-flash-latest',\n",
        "    google_api_key=userdata.get('google-api-key')\n",
        "  )\n",
        "  return llm"
      ],
      "metadata": {
        "id": "RdvIeDhN6RWC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"companypolicies.txt\")\n",
        "text = loader.load()\n",
        "text_chunk = text_splitter(text, 200, 20)"
      ],
      "metadata": {
        "id": "hNm3y-S_Abqi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vector backed retreiver**"
      ],
      "metadata": {
        "id": "ML8osA56VqSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectordb = Chroma.from_documents(text_chunk, embed_model())"
      ],
      "metadata": {
        "id": "JwhuxbLOBYbY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"input query :\")\n",
        "retreiver = vectordb.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFG2YSE_DhL0",
        "outputId": "23003cfc-f560-4b56-d7e2-146b156e93f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input query :smoke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retreiver.invoke(query)\n",
        "for doc in docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywRn6F_BVTbU",
        "outputId": "ed9c491a-6164-40f4-870c-1e4116e2e11d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smoke and to maintain the overall cleanliness of the premises.\n",
            "5.\tSmoking Policy\n",
            "We appreciate your cooperation in maintaining a smoke-free and safe environment for all.\n",
            "Designated Smoking Areas: Smoking is only permitted in designated smoking areas, as marked by appropriate signage. These areas have been chosen to minimize exposure to secondhand smoke and to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MMR SEARCH**\n",
        "this is a technique used to balance relevance and diversity of results. This helps in selecting a diverse set of relevant docs."
      ],
      "metadata": {
        "id": "hV1txZ4vV03F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
        "docs = retriever.invoke(query)\n"
      ],
      "metadata": {
        "id": "jqsPZbRsVdLI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZVRmVJuW3t1",
        "outputId": "66cbf793-c89b-48ea-c38d-bfa828e8a29b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smoke and to maintain the overall cleanliness of the premises.\n",
            "5.\tSmoking Policy\n",
            "those seeking treatment.\n",
            "or suspicious activities related to your mobile device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi Query retreiver**\n",
        " using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and then takes the unique union of these results to form a larger set of potentially relevant documents."
      ],
      "metadata": {
        "id": "wOF6Fuckj1Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ioch1wsxkfqgfLLgmd-6Rw/langchain-paper.pdf\")\n",
        "pdf_data = loader.load();"
      ],
      "metadata": {
        "id": "pXuNFaNcXJtK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_chunk = text_splitter(pdf_data, 500,20 )\n",
        "ids = vectordb.get()[\"ids\"]\n",
        "vectordb=Chroma.from_documents(pdf_chunk, embed_model())"
      ],
      "metadata": {
        "id": "bftu7JbBq0_O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "query='how is langchain used for large language models'\n",
        "\n",
        "retreiver = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    llm=llm()\n",
        ")"
      ],
      "metadata": {
        "id": "vfBsrirlsb-t"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "y17BdbWEuAY-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(query)\n",
        "for doc in docs:\n",
        "  print(\"*\"*30)\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj8FpXfVzjW7",
        "outputId": "cfa44133-3f9b-483f-89b1-a28e54b5a2e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************\n",
            "II. LANGCHAIN  \n",
            "LangChain , with its open -source essence, emerges as a \n",
            "promising solution, aiming to simplify the complex process of \n",
            "developing applications powered by large language models \n",
            "(LLMs) . This framework though the rapid delivery of building \n",
            "blocks and pre -built chains for building large language model \n",
            "applications shows the easy way developers can do it .\n",
            "******************************\n",
            "LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or \n",
            "incorporated seamlessly into a chain.  \n",
            "A memory system must support two fundamental \n",
            "actions: reading and writing. Remember that each chain has \n",
            "some fundamental execution mechanism that requires \n",
            "specific inputs. Some of these inputs are provided directly by \n",
            "the user, while others may be retrieve d from memory. In a\n",
            "******************************\n",
            "chain of prompts that the large language model (in this \n",
            "case, GPT -4) will process.  \n",
            "Step 6.  Model Response:  Dispatch the constructed input \n",
            "chain to the GPT -4 model for natural language \n",
            "understanding and generation.  The GPT -4 model \n",
            "generates a response based on the input and context.  \n",
            "Step 7.  Response to Streamlit:  Receive the response \n",
            "generated by the GPT -4 model and transmit it back to \n",
            "the Streamlit framework for display to the user.\n",
            "******************************\n",
            "Storing: List of chat messages: A history of all chat \n",
            "exchanges is behind each memory. Even if not all of \n",
            "these are immediately used, they must be preserved \n",
            "in some manner. A series of integrations for storing \n",
            "these conversation messages, ranging from in -\n",
            "memory lists to persistent databases, is a significant \n",
            "component of the LangChain memory module.  \n",
            "2. How state is queried  ? \n",
            "Querying: Data structures and algorithms on top of \n",
            "chat messages: Keeping track of chat messages is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KE0R8qcLzl3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}